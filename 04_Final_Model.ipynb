{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried all kinds of different pre-trained models and tuning methods, it's finally time to assemble our very own model based on what we've learned. This model will then serve as the foundation for our application that will be deployed using streamlit.\n",
    "\n",
    "*Note: To get the exact same model, please follow the steps in the previous notebooks, and pay special attention to the merged and deleted classes. If you're just looking for the model file, it's located at `models/\n",
    "model_filtered.h5`.*\n",
    "\n",
    "If you want to use the app right aways, you can do so with `streamlit run main.py`. This will use the model provided with this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/thewolfpandey/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in /home/thewolfpandey/.local/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: matplotlib in /home/thewolfpandey/.local/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in /home/thewolfpandey/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /home/thewolfpandey/.local/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: packaging in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /home/thewolfpandey/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: optree in /home/thewolfpandey/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.10.0)\n",
      "Requirement already satisfied: namex in /home/thewolfpandey/.local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/thewolfpandey/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow matplotlib seaborn scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../functions')\n",
    "from preprocessing import img_dataset_from_dir_and_split_train_val \n",
    "from modeling import build_model, unfreeze_model_and_clone\n",
    "from plot_functions import plot_training_metrics\n",
    "\n",
    "RSEED = 42\n",
    "DATASET_PATH = '../data/images/' # Path to the parent folder where the original data is stored\n",
    "TRAINING_IMAGES = '../data/train' # Path to training images\n",
    "TESTING_IMAGES = '../data/test' # Path to testing images\n",
    "CLASSES = [\n",
    "    'alternaria_leaf_spot',\n",
    "    'bacterial_blight',\n",
    "    'bacterial_spot',\n",
    "    'bacterial_wilt',\n",
    "    'black_measles',\n",
    "    'black_rot',\n",
    "    'blast',\n",
    "    'brown_spot',\n",
    "    'brown_streak_disease',\n",
    "    'citrus_greening',\n",
    "    'common_rust',\n",
    "    'early_blight',\n",
    "    'gray_leaf_spot',\n",
    "    'healthy',\n",
    "    'isariopsis_leaf_spot',\n",
    "    'late_blight',\n",
    "    'leaf_curl',\n",
    "    'leaf_mold',\n",
    "    'mosaic_disease',\n",
    "    'northern_leaf_blight',\n",
    "    'powdery_mildew',\n",
    "    'red_rot',\n",
    "    'spider_mites',\n",
    "    'target_spot',\n",
    "    'tungro',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Before working with Keras, it's good practice to clear the session\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ../data/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Next, the train-validation-split takes place. The train-test-split has already been done at the end of notebook 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_ds, val_ds \u001b[38;5;241m=\u001b[39m \u001b[43mimg_dataset_from_dir_and_split_train_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAINING_IMAGES\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/crops/Krishi_Mitra2.0/notebooks/../functions/preprocessing.py:55\u001b[0m, in \u001b[0;36mimg_dataset_from_dir_and_split_train_val\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimg_dataset_from_dir_and_split_train_val\u001b[39m(data_path):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Input: Path to parent directory where the images are stored in subfolders/classes\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Return training and validation data set at 80:20 ratio. Crops and resizes images to 224x224px. RSEED = 42\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcrop_to_aspect_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/image_dataset_utils.py:223\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/dataset_utils.py:537\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    536\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    539\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory ../data/train"
     ]
    }
   ],
   "source": [
    "# 2. Next, the train-validation-split takes place. The train-test-split has already been done at the end of notebook 1\n",
    "train_ds, val_ds = img_dataset_from_dir_and_split_train_val(TRAINING_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. We define the model and exclude the top layer\n",
    "\n",
    "model = keras.applications.EfficientNetB0(\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# Optional: Take a look at the model's architecture\n",
    "# mode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Here we build the model with 25 classes\n",
    "model = build_model(num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Unfreeze the top 10 layers\n",
    "unfrozen_model = unfreeze_model_and_clone(model)\n",
    "\n",
    "epochs = 10\n",
    "hist = unfrozen_model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Let's have a look at our metrics\n",
    "plot_training_metrics(hist)\n",
    "\n",
    "# ... and heatmap\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix using seaborn heatmap with green color palette\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Greens\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. We then have a look at the classification report\n",
    "\n",
    "def load_test(data_path):\n",
    "    ''' \n",
    "    Function needs filepath as parameter, it will create a validation dataset of 20% of the total df, \n",
    "    Needs an RSEED as global variable,\n",
    "    Image will be cropped to 1:1 and altered to 224 x 224\n",
    "    '''\n",
    "    image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_path,\n",
    "        image_size = (224, 224),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        label_mode = 'categorical',\n",
    "        shuffle = False\n",
    "    )\n",
    "    return image_dataset \n",
    "\n",
    "dataset_test_path = '../data/filtered_diseases/test_filtered/'\n",
    "\n",
    "test_ds = load_test(dataset_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the true labels from the test dataset\n",
    "y_true = []\n",
    "for filepath in test_ds.file_paths:\n",
    "    label = os.path.basename(os.path.dirname(filepath))\n",
    "    y_true.append(label)\n",
    "\n",
    "# Extract unique class labels from your training data\n",
    "classes = sorted(set(y_true))\n",
    "\n",
    "# Step 2: Convert true labels to indices using the same mapping used during training\n",
    "class_to_index = {cls: i for i, cls in enumerate(classes)}\n",
    "y_true_indices = np.array([class_to_index[label] for label in y_true])\n",
    "\n",
    "# Step 3: Use your model to make predictions on the test dataset\n",
    "y_pred_probabilities = patho_model_10.predict(test_ds)\n",
    "\n",
    "# Step 4: Convert the predicted class probabilities to class labels\n",
    "y_pred_indices = np.argmax(y_pred_probabilities, axis=1)\n",
    "y_pred = [classes[i] for i in y_pred_indices]\n",
    "\n",
    "# Step 5: Generate the classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. We can have a look at the different class probabilities\n",
    "\n",
    "def display_class_probabilities(model, img_path, class_names):\n",
    "    # Load and preprocess the input data\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) \n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    # Get class probabilities\n",
    "    probabilities = model.predict(img_array)[0]\n",
    "\n",
    "    # Display class probabilities with class names\n",
    "    for class_name, prob in zip(class_names, probabilities):\n",
    "        print(f\"{class_name}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_class_probabilities(model_filtered.h5, image_path, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Once we're happy with the model, we save it\n",
    "unfrozen_model.save('patho_model_10.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
